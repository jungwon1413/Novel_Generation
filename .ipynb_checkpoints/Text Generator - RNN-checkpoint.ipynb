{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generator using RNN\n",
    "## Novel Generator based on custom fantasy books.\n",
    "<br> At least 5000 epochs are required before the generated text starts sounding coherent.\n",
    "<br> *(This model is inspired by many examples, such as Keras samples, or tensorflow tutorials.)*\n",
    "<br>\n",
    "<br> Few notices:\n",
    "<br>- This script is <b>NOT</b> designed for large text.\n",
    "<br>- This script is designed for ANY language. (It works for my language, at least.)\n",
    "<br>- It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jungw\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import chardet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.seq2seq import sequence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Details\n",
    "- \\_\\_init\\_\\_: Load file with encoding option.\n",
    "    - Default Encoding: None\n",
    "- Slice_Data: Split text data into characters (This can be words if modified correctly)\n",
    "- Data2idx: Transform text data into index\n",
    "- Build_Data: Generate sequences of transformed text data\n",
    "    - Default Stride: 1\n",
    "- LSTM_Cell: A LSTM cell, only to use at rnn.MultiRNNCell\n",
    "- Make_Text: A random text generator, based on text data\n",
    "- Elapsed: To calculate time elapsed\n",
    "- Save_Model: Save current Neural Network model\n",
    "    - Default Global_step: 1000\n",
    "- Load_Model: Load specified model file\n",
    "- Prepare_Model: Generate model structure for training (X, Y, weight, etc.)\n",
    "- Train: Start training model with epoch number followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGen:\n",
    "    def __init__(self, full_filename, encode=False):\n",
    "        try:\n",
    "            read_file = open(full_filename, 'rb').read()\n",
    "            if encode is True:\n",
    "                encode_type = chardet.detect(read_file)['encoding']\n",
    "                print(encode_type)\n",
    "            else:\n",
    "                encode_type = None\n",
    "            # text = open(full_filename, encoding=None).read().lower()\n",
    "            text = open(full_filename, encoding=encode_type).read()\n",
    "            self.text = text\n",
    "        except:\n",
    "            self.text = False\n",
    "\n",
    "    def Slice_Data(self):\n",
    "        print(\"Word Slicing...\")\n",
    "        self.char_set = sorted(list(set(self.text)))    # character split\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.char_set))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.char_set))\n",
    "\n",
    "    def Data2idx(self):\n",
    "        idx_text = []\n",
    "        for char in self.text:\n",
    "            idx_text.append(self.char_indices[char])\n",
    "        self.text = idx_text\n",
    "\n",
    "    def Build_Data(self, seq_len, stride = 1):\n",
    "        print(\"Generating Number Index...\")\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for i in range(0, len(self.text) - self.seq_len, stride):\n",
    "            fill_x = seq_len - len(self.text[i : i+seq_len])\n",
    "            fill_y = seq_len - len(self.text[i+1 : i+seq_len - 1])\n",
    "            \n",
    "            x_text = self.text[i : i+seq_len]\n",
    "            y_text = self.text[i+1 : i+seq_len - 1]\n",
    "            \n",
    "            if fill_x is not 0:\n",
    "                x_text.extend([0 for i in range(fill_x)])\n",
    "            elif fill_y is not 0:\n",
    "                y_text.extend([0 for i in range(fill_y)])\n",
    "\n",
    "            x_data.append(x_text)\n",
    "            y_data.append(y_text)\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def LSTM_Cell(hidden_size):\n",
    "        # Make a lstm cell with hidden_size (each unit output vector size)\n",
    "        cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "        return cell\n",
    "\n",
    "    def Make_Text(self):\n",
    "        self.results = sess.run(self.outputs, feed_dict={self.X: self.x_data})\n",
    "        for j, result in enumerate(self.results):\n",
    "            index = np.argmax(result, axis=1)\n",
    "            if j is 0:  # print all for the first result to make a sentence\n",
    "                print(''.join([self.indices_char[t] for t in index]), end='')\n",
    "            else:\n",
    "                print(self.indices_char[index[-1]], end='')\n",
    "\n",
    "    def Elapsed(start, end):\n",
    "        total = end - start\n",
    "        m, s = divmod(total, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        \n",
    "        if m == 0 and h == 0:\n",
    "            print(\"Time Elapsed: {:>3d} sec\".format(int(s)))\n",
    "        elif m != 0 and h == 0:\n",
    "            print(\"Time Elapsed: {:>3d} min {:>3d} sec\".format(int(m), int(s)))\n",
    "        else:\n",
    "            print(\"Time Elapsed: {:>3d} hour {:>3d} min {:>3d} sec\".format(int(h), int(m), int(s)))\n",
    "            \n",
    "    def Save_Model(saver, sess, filepath, step=1000):\n",
    "        save_model = saver.save(sess, filepath, global_step=step)\n",
    "        print(\"Model saved in path: %s\" % save_model)\n",
    "        \n",
    "    def Load_Model(saver, sess, filepath):\n",
    "        saver.restore(sess, filepath)\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    def Prepare_Model(self, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # load data\n",
    "        TextGen.Slice_Data(self)\n",
    "        TextGen.Data2idx(self)\n",
    "        TextGen.Build_Data(self, seq_len)\n",
    "\n",
    "        self.data_dim = len(self.char_set)\n",
    "        self.hidden_size = len(self.char_set)\n",
    "        self.num_classes = len(self.char_set)\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "        print(\"Text length: %s\" % len(self.text))\n",
    "        print(\"Character set length: %s\" % self.data_dim)    # length check\n",
    "        print(\"Dataset X length: %s\" % len(self.x_data))    # dataset shape check\n",
    "        print(\"Dataset Y length: %s\" % len(self.y_data))\n",
    "\n",
    "        self.batch_size = len(self.x_data)\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, [None, self.seq_len])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, self.seq_len])\n",
    "\n",
    "        # One-hot encoding\n",
    "        X_one_hot = tf.one_hot(self.X, self.num_classes)\n",
    "\n",
    "        # Make a lstm cell with hidden_size (each unit output vector size)\n",
    "        lstm = TextGen.LSTM_Cell(self.hidden_size)\n",
    "        multi_cells = rnn.MultiRNNCell([lstm for _ in range(2)], state_is_tuple=True)\n",
    "\n",
    "        # outputs: unfolding size x hidden size, state = hidden size\n",
    "        outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
    "\n",
    "        # FC layer\n",
    "        X_for_fc = tf.reshape(outputs, [-1, self.hidden_size])\n",
    "        outputs = fully_connected(X_for_fc, self.num_classes, activation_fn=None)\n",
    "\n",
    "        # reshape out for sequence_loss\n",
    "        self.outputs = tf.reshape(outputs, [self.batch_size, self.seq_len, self.num_classes])\n",
    "\n",
    "        # All weights are 1 (equal weights)\n",
    "        weights = tf.ones([self.batch_size, self.seq_len])\n",
    "\n",
    "        loss = sequence_loss(logits=self.outputs, targets=self.Y, weights=weights)\n",
    "        self.mean_loss = tf.reduce_mean(loss)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.mean_loss)\n",
    "\n",
    "    def Train(self, sess, epoch, save_at):\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('=' * 20, \"{:^20}\".format(\"Training Start\"), '=' * 20)\n",
    "\n",
    "        self.iter_loss = []\n",
    "        self.elapsed_loss = []\n",
    "        self.savepath = save_at\n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(epoch):\n",
    "            _, l, results = sess.run(\n",
    "                [self.train_op, self.mean_loss, self.outputs],\n",
    "                feed_dict={self.X: self.x_data, self.Y: self.y_data})\n",
    "            for j, result in enumerate(results):\n",
    "                index = np.argmax(result, axis=1)\n",
    "                \n",
    "                if i % 100 == 0 and j == 0:\n",
    "                    print(\"\\n At step\", i, ':',\n",
    "                        ''.join([self.indices_char[t] for t in index]))\n",
    "                    print('Loss:', l)\n",
    "                    end = time.time()\n",
    "                    TextGen.Elapsed(start, end)\n",
    "            if i % 1000 == 0:\n",
    "                TextGen.Save_Model(saver, sess, self.savepath)            \n",
    "            self.iter_loss.append(l)    # Iteration & Loss\n",
    "            self.elapsed_loss.append([start-end, l])    # Elapsed Time & Loss\n",
    "\n",
    "\n",
    "        print('=' * 20, \"{:^20}\".format(\"Training Complete\"), '=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Slicing...\n",
      "Generating Number Index...\n",
      "Text length: 12030\n",
      "Character set length: 75\n",
      "Dataset X length: 11990\n",
      "Dataset Y length: 11990\n",
      "====================    Training Start    ====================\n",
      "\n",
      " At step 0 : WW--Wh ,EEWWWWWWWWWWLLjEEEEELLLLLEEEEEWE\n",
      "Loss: 4.317299\n",
      "Time Elapsed:   1 sec\n",
      "Model saved in path: C:/Users/jungw/OneDrive/문서/GitHub/Text_Generation/rnn_text.ckpt-1000\n",
      "\n",
      " At step 100 : niort =p  siulnati_d' ueteeetNN_LeG_LeeE\n",
      "Loss: 1.1736252\n",
      "Time Elapsed:   1 min  14 sec\n",
      "\n",
      " At step 200 : neort =f gstansaron['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.62029946\n",
      "Time Elapsed:   2 min  30 sec\n",
      "\n",
      " At step 300 : neort tu gs.anviron['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.48378268\n",
      "Time Elapsed:   3 min  47 sec\n",
      "\n",
      " At step 400 : neort tp os.anviron['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.41775998\n",
      "Time Elapsed:   5 min   3 sec\n",
      "\n",
      " At step 500 : nsort lsros.anviron['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.5007582\n",
      "Time Elapsed:   6 min  19 sec\n",
      "\n",
      " At step 600 : nport nsros.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.3689926\n",
      "Time Elapsed:   7 min  35 sec\n",
      "\n",
      " At step 700 : zeort tsros.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.3924841\n",
      "Time Elapsed:   8 min  52 sec\n",
      "\n",
      " At step 800 : nport lsros.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.35752642\n",
      "Time Elapsed:  10 min   7 sec\n",
      "\n",
      " At step 900 : nport nsros.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.3392734\n",
      "Time Elapsed:  11 min  23 sec\n",
      "\n",
      " At step 1000 : nsort tsros.anviron['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.2978862\n",
      "Time Elapsed:  12 min  39 sec\n",
      "Model saved in path: C:/Users/jungw/OneDrive/문서/GitHub/Text_Generation/rnn_text.ckpt-1000\n",
      "\n",
      " At step 1100 : neort nsros.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.28779465\n",
      "Time Elapsed:  13 min  56 sec\n",
      "\n",
      " At step 1200 : neort tsios.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.2978096\n",
      "Time Elapsed:  15 min  12 sec\n",
      "\n",
      " At step 1300 : neort nsros.anviron['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.40895534\n",
      "Time Elapsed:  16 min  29 sec\n",
      "\n",
      " At step 1400 : neort ts'os.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.46046537\n",
      "Time Elapsed:  17 min  44 sec\n",
      "\n",
      " At step 1500 : neort nsros.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.29007003\n",
      "Time Elapsed:  19 min   1 sec\n",
      "\n",
      " At step 1600 : nsort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.2605887\n",
      "Time Elapsed:  20 min  17 sec\n",
      "\n",
      " At step 1700 : neort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.2651456\n",
      "Time Elapsed:  21 min  35 sec\n",
      "\n",
      " At step 1800 : nsort tsros.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.28761727\n",
      "Time Elapsed:  22 min  51 sec\n",
      "\n",
      " At step 1900 : nport ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.27437967\n",
      "Time Elapsed:  24 min   8 sec\n",
      "\n",
      " At step 2000 : neort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.27217135\n",
      "Time Elapsed:  25 min  24 sec\n",
      "Model saved in path: C:/Users/jungw/OneDrive/문서/GitHub/Text_Generation/rnn_text.ckpt-1000\n",
      "\n",
      " At step 2100 : nsort ls\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.29415628\n",
      "Time Elapsed:  26 min  42 sec\n",
      "\n",
      " At step 2200 : zsort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.29986656\n",
      "Time Elapsed:  27 min  57 sec\n",
      "\n",
      " At step 2300 : neort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.32526755\n",
      "Time Elapsed:  29 min  13 sec\n",
      "\n",
      " At step 2400 : neort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.28659704\n",
      "Time Elapsed:  30 min  30 sec\n",
      "\n",
      " At step 2500 : neort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.31501707\n",
      "Time Elapsed:  31 min  45 sec\n",
      "\n",
      " At step 2600 : nsort ns\n",
      "os.environ['TF_CPP_MIN_LOG_LEV\n",
      "\n",
      "Loss: 0.2635982\n",
      "Time Elapsed:  33 min   1 sec\n",
      "\n",
      " At step 2700 : neort ts\n",
      "os.environ['TF_CPP_MIN_LOG_LEVE\n",
      "Loss: 0.3550472\n",
      "Time Elapsed:  34 min  16 sec\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    code_gen = TextGen(\"SAMPLE.py\")\n",
    "    code_gen.Prepare_Model(40)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    save_at = \"C:/Users/jungw/OneDrive/문서/GitHub/Text_Generation/rnn_text.ckpt\"\n",
    "    epoch = 10000\n",
    "    \n",
    "    code_gen.Train(sess, epoch, save_at)\n",
    "    \n",
    "    code_gen.Make_Text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
