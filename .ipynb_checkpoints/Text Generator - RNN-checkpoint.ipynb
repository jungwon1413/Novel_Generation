{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generator using RNN\n",
    "## Novel Generator based on custom fantasy books.\n",
    "<br> At least 20 epochs are required before the generated text starts sounding coherent.\n",
    "<br> *(This model is inspired by many examples, such as Keras samples, or tensorflow tutorials.)*\n",
    "<br>\n",
    "<br> Few notices:\n",
    "<br>- This script is designed for <b>EXTREMELY</b> large text. (>> 1M characters)\n",
    "<br>- This script is designed for ANY language. (It works for my language, at least.)\n",
    "<br>- This script will use <b>8GB+</b> (this number depends on your dataset) of RAM when operated. (Whether you run with GPU or not.)\n",
    "<br>- However, it is recommended to run this script on GPU, as recurrent networks are quite computationally intensive.\n",
    "<br>- If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jungw\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import chardet\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path, filename):\n",
    "    # path = get_file('path/to/text/file.txt', origin=None)\n",
    "    # filename = 'D:/Seed_Downloads/Novel Dataset/Asian fantasy/NOVEL_01001.txt'\n",
    "    if path[-1] is '/':\n",
    "        file = path + filename\n",
    "    else:\n",
    "        file = path + '/' + filename\n",
    "\n",
    "    read_file = open(file, 'rb').read()\n",
    "    # encode_type = chardet.detect(read_file)['encoding']\n",
    "    # print(encode_type)    # None이었다...\n",
    "    # text = open(file, encoding=encode_type).read().lower()\n",
    "    text = open(file, encoding=None).read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2928103\n",
      "First 100 characters: \n",
      "제 목:[검마전/ sword & magic story]-- 001.\n",
      "\n",
      "< 검 마 전 : sword & magic story >\n",
      "\n",
      "눈꺼풀이 무겁다. 머리도 띵하고. 눈을 떠야하는데.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "text = load_file('D:/Seed_Downloads/Novel Dataset/Asian fantasy/', 'NOVEL_01001.txt')\n",
    "\n",
    "print('First 100 characters: {}'.format('\\n' + text[:100]))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>문제점</b>\n",
    "단어를 나눌시 공백문자나 개행문자가 사라지는 현상이 있다. 따라서 다음과 같이 처리한다.\n",
    "- 공백문자: ' SPACE '로 대체한다.\n",
    "- 개행문자: ' ENTER '로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indicers(text):\n",
    "    temp_text = text\n",
    "    \n",
    "    temp_text = temp_text.replace(\" \", \" SPACE \")\n",
    "    temp_text = temp_text.replace(\"\\n\", \" ENTER \")\n",
    "    \n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed first 100 characters: \n",
      "제 SPACE 목:[검마전/ SPACE sword SPACE & SPACE magic SPACE story]-- SPACE 001. ENTER  ENTER < SPACE 검 SPA\n"
     ]
    }
   ],
   "source": [
    "text = remove_indicers(text)\n",
    "\n",
    "print('Fixed first 100 characters: {}'.format('\\n' + text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **단어 나누기**\n",
    "konlpy모듈의 Twitter에서 제공하는 morphs메서드를 이용해서 단어를 추출해낸다.\n",
    "<br> 추출 후 다음 기능을 생성한다.\n",
    "- word_indices: 단어를 index로 변환\n",
    "- indices_word: index를 단어로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_word(text_replace):\n",
    "    twitter = Twitter()\n",
    "    text_split = twitter.morphs(text_replace)\n",
    "    words = sorted(list(set(text_split)))\n",
    "    min_word = min(words, key=len)\n",
    "    max_word = max(words, key=len)\n",
    "\n",
    "    print('Total words:', len(words))\n",
    "    print('min word is: {} with length of {}'.format(min_word, len(min_word)))\n",
    "    print('max word is: {} with length of {}'.format(max_word, len(max_word)))\n",
    "\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "    \n",
    "    return text_split, words, word_indices, indices_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 30211\n",
      "min word is: ! with length of 1\n",
      "max word is: \"@#%*(*^$#&*#$\" with length of 15\n",
      "Split check: 느낌\n",
      "Word check: ..]\n"
     ]
    }
   ],
   "source": [
    "text_split, words, word_indices, indices_word = split_to_word(text)\n",
    "\n",
    "print('Split check: {}'.format(text_split[100]))\n",
    "print('Word check: {}'.format(words[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **문장 생성**\n",
    "라인별로 문장을 생성한다. 문장별로 단어 슬라이싱을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_create(text_split):\n",
    "    sentences = []\n",
    "    temp = \"\"\n",
    "\n",
    "    twitter = Twitter()\n",
    "    \n",
    "    for i, word in enumerate(text_split):\n",
    "        temp = temp + word + ' '\n",
    "        if word == 'ENTER':\n",
    "            temp = twitter.morphs(temp)\n",
    "            sentences.append(temp)\n",
    "            temp = \"\"\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print('Total number of Sentences: {}'.format(len(sentences)))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Sentences: 109452\n",
      "Sentence check: ['제', 'SPACE', '목', ':[', '검', '마전', '/', 'SPACE', 'sword', 'SPACE', '&', 'SPACE', 'magic', 'SPACE', 'story', ']--', 'SPACE', '001', '.', 'ENTER']\n"
     ]
    }
   ],
   "source": [
    "sentences = sentence_create(text_split)\n",
    "\n",
    "print('Sentence check: {}'.format(sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Input Sequence생성**\n",
    "문장을 기준으로 Input에 들어갈 시퀀스를 생성한다. 각 시퀀스는 n개의 단어로 구성되어있다.\n",
    "<br>여기서는 최대 길이의 문장(단어 개수 기준)을 기준으로 문장의 길이를 생성했다.\n",
    "<br>예시 (최대길이: 25단어)\n",
    "- 1번문장: 0 ~ 25번째\n",
    "- 2번문장: 3 ~ 28번째\n",
    "- 3번문장: 6 ~ 31번째\n",
    "- ...\n",
    "- n번문장: 3(n-1) ~ 3(n-1)+25번째"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_sequence(sentences):\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "    longest_sentence = max(sentences)\n",
    "    maxlen = len(longest_sentence)\n",
    "    step = 3\n",
    "    # next_words = []\n",
    "    sentence_data = []\n",
    "    seq_count_ = (len(text_split) - maxlen) // step\n",
    "\n",
    "    for i in range(0, len(text_split) - maxlen, step):\n",
    "        sentence_data.append(text_split[i: i + maxlen])    # nth char ~ n+maxlen char = Sentence\n",
    "        # next_words.append(text_split[i + maxlen])    # next_chars = from ith char ~ end\n",
    "    \n",
    "    return sentence_data, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 682030\n",
      "101th sentence length: 26\n",
      "max length: 26\n"
     ]
    }
   ],
   "source": [
    "sentence_data, maxlen = generate_input_sequence(sentences)\n",
    "\n",
    "print('nb sequences:', len(sentence_data))\n",
    "print('101th sentence length: {}'.format(len(sentence_data[100])))\n",
    "print('max length: {}'.format(maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **메모리 문제**\n",
    "텍스트 데이터가 너무 크기 때문에 전체를 처리하려면 얼마나 많은 메모리 공간이 필요한지 추측해보았다.\n",
    "<br>기준은 (문장개수 x 최대길이 x 단어개수) x 4bit / (2의32승)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected memory to prepare: 19133036306.42857GB\n"
     ]
    }
   ],
   "source": [
    "Memory_to_use = len(sentence_data) * maxlen * len(words) / (2^30)\n",
    "\n",
    "print('Expected memory to prepare: {}GB'.format(Memory_to_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **시퀀스 체크**\n",
    "시퀀스가 제대로 생성이 되었는지 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence check...\n",
      "Sentence 0:\n",
      "['귀', '를', 'SPACE', '기울여', '도', 'SPACE', '바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은']\n",
      "Sentence 1:\n",
      "['기울여', '도', 'SPACE', '바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은', '거', '?', 'SPACE']\n",
      "Sentence 2:\n",
      "['바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은', '거', '?', 'SPACE', '으아', '..!', 'SPACE']\n"
     ]
    }
   ],
   "source": [
    "print('Sequence check...')\n",
    "for i in range(3):\n",
    "    print('Sentence {}:\\n{}'.format(i, sentence_data[100+i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **단어 → Index 변환**\n",
    "실제로 처리할때는 String으로서의 단어가 아닌 int로서의 단어, 즉 index가 필요하다.\n",
    "<br>따라서 단어를 Index로 변환해 줄 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_indices(text_split):\n",
    "    indice_list = []\n",
    "    for i, word in enumerate(text_split):\n",
    "        add_indice = word_indices[word]\n",
    "        indice_list.append(add_indice)\n",
    "        \n",
    "    return indice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sentence sample: [110, 1008, 1308, 1008, 25, 1008, 1199, 1008, 1303, 1012, 1008, 114, 66, 1007, 1007, 989, 1008, 2873, 1008, 10268, 1008, 22969, 1008, 985, 1008, 1308]\n",
      "length of encoded sentence list: 682030\n"
     ]
    }
   ],
   "source": [
    "# encoded_text = word_to_indices(text_split)\n",
    "# encoded_next_word = word_to_indices(next_words)\n",
    "\n",
    "encoded_sentences = []\n",
    "for sentence in sentence_data:\n",
    "    encoded_sentences.append(word_to_indices(sentence))\n",
    "\n",
    "# print('Encoded text sample: {}'.format(str(encoded_text[:20])))\n",
    "# print('length of encoded list: {}'.format(len(encoded_text)))\n",
    "# print('Encoded trigger word sample: {}'.format(str(encoded_next_word[:20])))\n",
    "# print('length of encoded list: {}'.format(len(encoded_next_word)))\n",
    "print('Encoded sentence sample: {}'.format(str(encoded_sentences[2])))\n",
    "print('length of encoded sentence list: {}'.format(len(encoded_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Variable List**\n",
    "- text: raw text\n",
    "- text_split: word기준으로 나뉜 text\n",
    "- words: 단어 corpus\n",
    "- word_indices: 단어를 index로\n",
    "- indices_word: index를 단어로\n",
    "- sentences: 문장 기준으로 나뉜 text\n",
    "- sentence_data: 문장 기준으로 생성된 sequence\n",
    "- maxlen: 문장 최대 길이(int)\n",
    "- encoded_sentences: index(int)로 변경된 sequence 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = len(words)\n",
    "hidden_size = len(words)\n",
    "num_classes = len(words)\n",
    "sequence_length = maxlen\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 문제\n",
    "- 이슈: 너무 크다 (파일사이즈)\n",
    "    - 모든 방법을 이용해봤지만 여전히 너무 크다\n",
    "- 결론: Iteration을 통해 Feeding을 실시간으로 하는 방법을 사용한다\n",
    "\n",
    "<br>1. 원하는 Size의 Input/Output배열을 만든다\n",
    "<br>2. 실시간 생성을 통한 feeding 진행\n",
    "- 0~9999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- 10000 ~ 19999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- 20000 ~ 29999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- ... (이하 생략)\n",
    "- 680000 ~ 682030 생성\n",
    "<br> Train (0 Epoch)\n",
    "\n",
    "<br>3. Predict\n",
    "<br>4. Loss 산출\n",
    "<br>5. 2번의 과정을 다시 진행\n",
    "<br>\n",
    "<br>\n",
    "<br> LSTM의 대략적인 구조는 다음과 같다.\n",
    "<br> [Not-so-detailed Details]\n",
    "<br> Input => LSTM_cell => predicted output at t\n",
    "<br> => LSTM_cell => LAST predicted output at t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
