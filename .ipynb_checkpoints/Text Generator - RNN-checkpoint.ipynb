{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generator using RNN\n",
    "## Novel Generator based on custom fantasy books.\n",
    "<br> At least 20 epochs are required before the generated text starts sounding coherent.\n",
    "<br> *(This model is inspired by many examples, such as Keras samples, or tensorflow tutorials.)*\n",
    "<br>\n",
    "<br> Few notices:\n",
    "<br>- This script is designed for <b>EXTREMELY</b> large text. (>> 1M characters)\n",
    "<br>- This script is designed for ANY language. (It works for my language, at least.)\n",
    "<br>- This script will use <b>8GB+</b> (this number depends on your dataset) of RAM when operated. (Whether you run with GPU or not.)\n",
    "<br>- However, it is recommended to run this script on GPU, as recurrent networks are quite computationally intensive.\n",
    "<br>- If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jungw\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import random    << use np.random instead.\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import chardet\n",
    "\n",
    "from collections import Counter\n",
    "from konlpy.tag import Twitter\n",
    "from scipy.sparse import *\n",
    "from tensorflow.python.keras.callbacks import LambdaCallback    # What is Lambda Callback??\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Activation\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.utils import get_file    # Do I have to use get_file instead of direct load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path, filename):\n",
    "    # path = get_file('path/to/text/file.txt', origin=None)\n",
    "    # filename = 'D:/Seed_Downloads/Novel Dataset/Asian fantasy/NOVEL_01001.txt'\n",
    "    if path[-1] is '/':\n",
    "        file = path + filename\n",
    "    else:\n",
    "        file = path + '/' + filename\n",
    "\n",
    "    read_file = open(file, 'rb').read()\n",
    "    encode_type = chardet.detect(read_file)['encoding']\n",
    "    text = open(file, encoding=encode_type).read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2928103\n",
      "First 100 characters: \n",
      "제 목:[검마전/ sword & magic story]-- 001.\n",
      "\n",
      "< 검 마 전 : sword & magic story >\n",
      "\n",
      "눈꺼풀이 무겁다. 머리도 띵하고. 눈을 떠야하는데.\n"
     ]
    }
   ],
   "source": [
    "text = load_file('D:/Seed_Downloads/Novel Dataset/Asian fantasy/', 'NOVEL_01001.txt')\n",
    "\n",
    "print('First 100 characters: {}'.format('\\n' + text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indicers(text):\n",
    "    temp_text = text\n",
    "    \n",
    "    temp_text = temp_text.replace(\" \", \" SPACE \")\n",
    "    temp_text = temp_text.replace(\"\\n\", \" ENTER \")\n",
    "    \n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed first 100 characters: \n",
      "제 SPACE 목:[검마전/ SPACE sword SPACE & SPACE magic SPACE story]-- SPACE 001. ENTER  ENTER < SPACE 검 SPA\n"
     ]
    }
   ],
   "source": [
    "text = remove_indicers(text)\n",
    "\n",
    "print('Fixed first 100 characters: {}'.format('\\n' + text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_word(text_replace):\n",
    "    twitter = Twitter()\n",
    "    text_split = twitter.morphs(text_replace)\n",
    "    words = sorted(list(set(text_split)))\n",
    "    min_word = min(words, key=len)\n",
    "    max_word = max(words, key=len)\n",
    "\n",
    "    print('Total words:', len(words))\n",
    "    print('min word is: {} with length of {}'.format(min_word, len(min_word)))\n",
    "    print('max word is: {} with length of {}'.format(max_word, len(max_word)))\n",
    "\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "    \n",
    "    return text_split, words, word_indices, indices_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 30211\n",
      "min word is: ! with length of 1\n",
      "max word is: \"@#%*(*^$#&*#$\" with length of 15\n",
      "Split check: 느낌\n",
      "Word check: ..]\n"
     ]
    }
   ],
   "source": [
    "text_split, words, word_indices, indices_word = split_to_word(text)\n",
    "\n",
    "print('Split check: {}'.format(text_split[100]))\n",
    "print('Word check: {}'.format(words[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_create(text_split):\n",
    "    sentences = []\n",
    "    temp = \"\"\n",
    "\n",
    "    twitter = Twitter()\n",
    "    \n",
    "    for i, word in enumerate(text_split):\n",
    "        temp = temp + word + ' '\n",
    "        if word == 'ENTER':\n",
    "            temp = twitter.morphs(temp)\n",
    "            sentences.append(temp)\n",
    "            temp = \"\"\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print('Total number of Sentences: {}'.format(len(sentences)))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Sentences: 109452\n",
      "Sentence check: ['제', 'SPACE', '목', ':[', '검', '마전', '/', 'SPACE', 'sword', 'SPACE', '&', 'SPACE', 'magic', 'SPACE', 'story', ']--', 'SPACE', '001', '.', 'ENTER']\n"
     ]
    }
   ],
   "source": [
    "sentences = sentence_create(text_split)\n",
    "\n",
    "print('Sentence check: {}'.format(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_sequence(sentences):\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "    longest_sentence = max(sentences)\n",
    "    maxlen = len(longest_sentence)\n",
    "    step = 3\n",
    "    next_words = []\n",
    "    sentence_data = []\n",
    "    seq_count_ = (len(text_split) - maxlen) // step\n",
    "\n",
    "    for i in range(0, len(text_split) - maxlen, step):\n",
    "        sentence_data.append(text_split[i: i + maxlen])    # nth char ~ n+maxlen char = Sentence\n",
    "        next_words.append(text_split[i + maxlen])    # next_chars = from ith char ~ end\n",
    "    \n",
    "    return sentence_data, next_words, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 682030\n",
      "101th sentence length: 26\n",
      "max length: 26\n"
     ]
    }
   ],
   "source": [
    "sentence_data, next_words, maxlen = generate_input_sequence(sentences)\n",
    "\n",
    "print('nb sequences:', len(sentence_data))\n",
    "print('101th sentence length: {}'.format(len(sentence_data[100])))\n",
    "print('max length: {}'.format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected memory to prepare: 19133036306.42857GB\n"
     ]
    }
   ],
   "source": [
    "Memory_to_use = len(sentence_data) * maxlen * len(words) / (2^30)\n",
    "\n",
    "print('Expected memory to prepare: {}GB'.format(Memory_to_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence check...\n",
      "Sentence 0:\n",
      "['귀', '를', 'SPACE', '기울여', '도', 'SPACE', '바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은']\n",
      "Sentence 1:\n",
      "['기울여', '도', 'SPACE', '바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은', '거', '?', 'SPACE']\n",
      "Sentence 2:\n",
      "['바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은', '거', '?', 'SPACE', '으아', '..!', 'SPACE']\n"
     ]
    }
   ],
   "source": [
    "print('Sequence check...')\n",
    "for i in range(3):\n",
    "    print('Sentence {}:\\n{}'.format(i, sentence_data[100+i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_indices(text_split):\n",
    "    indice_list = []\n",
    "    for i, word in enumerate(text_split):\n",
    "        add_indice = word_indices[word]\n",
    "        indice_list.append(add_indice)\n",
    "        \n",
    "    return indice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [23380, 1008, 11442, 987, 2873, 10386, 110, 1008, 1308, 1008, 25, 1008, 1199, 1008, 1303, 1012, 1008, 114, 66, 1007]\n",
      "length of encoded list: 2046114\n",
      "Encoded trigger word sample: [1008, 985, 1008, 1199, 1008, 1007, 1008, 66, 7990, 28452, 6645, 9258, 6794, 23511, 5959, 24023, 1008, 1008, 998, 21317]\n",
      "length of encoded list: 682030\n"
     ]
    }
   ],
   "source": [
    "encoded_text = word_to_indices(text_split)\n",
    "encoded_next_word = word_to_indices(next_words)\n",
    "\n",
    "print('Encoded text sample: {}'.format(str(encoded_text[:20])))\n",
    "print('length of encoded list: {}'.format(len(encoded_text)))\n",
    "print('Encoded trigger word sample: {}'.format(str(encoded_next_word[:20])))\n",
    "print('length of encoded list: {}'.format(len(encoded_next_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 문제\n",
    "- 이슈: 너무 크다 (파일사이즈)\n",
    "    - 모든 방법을 이용해봤지만 여전히 너무 크다\n",
    "- 결론: Iteration을 통해 Feeding을 실시간으로 하는 방법을 사용한다\n",
    "\n",
    "<br>1. 원하는 Size의 Input/Output배열을 만든다\n",
    "<br>2. 실시간 생성을 통한 feeding 진행\n",
    "- 0~9999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- 10000 ~ 19999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- 20000 ~ 29999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- ... (이하 생략)\n",
    "- 680000 ~ 682030 생성\n",
    "<br> Train (0 Epoch)\n",
    "\n",
    "<br>3. Predict\n",
    "<br>4. Loss 산출\n",
    "<br>5. 2번의 과정을 다시 진행\n",
    "<br>\n",
    "<br>\n",
    "<br> LSTM의 대략적인 구조는 다음과 같다.\n",
    "<br> [Not-so-detailed Details]\n",
    "<br> Input => LSTM_cell => predicted output at t\n",
    "<br> => LSTM_cell => LAST predicted output at t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡해서 따로 빼냈다.\n",
    "empty_input = np.zeros((10000, maxlen, wordlen), dtype=np.bool)\n",
    "empty_output = np.zeros((10000, wordlen), dtype=np.bool)\n",
    "x_list = []\n",
    "y_list = []\n",
    "iter_count = 0\n",
    "\n",
    "print('You will iterate {}times for each epoch.'.format(len(sentence_data) // 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row, sentence in enumerate(sentence_data):\n",
    "    if (row % 10000) is 0:\n",
    "        if row is not 0:\n",
    "            iter_count += 1\n",
    "\n",
    "            print('Iteration #{} processed...'.format(iter_count))\n",
    "\n",
    "            empty_input = np.zeros((10000, maxlen, wordlen), dtype=np.bool)\n",
    "            empty_output = np.zeros((10000, wordlen), dtype=np.bool)\n",
    "\n",
    "    else:\n",
    "        row_count = row - iter_count * 10000\n",
    "        output_index = encoded_next_word[row]\n",
    "        empty_output[row_count, output_index] = 1\n",
    "\n",
    "        for col, word in enumerate(sentence):\n",
    "            input_index = word_indices[word]\n",
    "            empty_input[row_count, col, input_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "[ x1, x2, ..., x26] (Line 1)\n",
    "[ x1, x2, ..., x26] (Line 2)\n",
    "...\n",
    "[ x1, x2, ..., x26] (Line 109452)\n",
    "\n",
    "\n",
    "---------------------------------\n",
    "[2n개의 경우(26)]\n",
    "\n",
    "1)\n",
    "[x1, x2, x3, x4, x5, x6] (Line 1)\n",
    "[x1, x2, x3, x4, x5, x6] (Line 2)\n",
    "...\n",
    "[x1, x2, x3, x4, x5, x6] (Line 109452)\n",
    "\n",
    "2)\n",
    "[x3, x4, x5, x6, x7, x8] (Line 1)\n",
    "[x3, x4, x5, x6, x7, x8] (Line 2)\n",
    "...\n",
    "[x3, x4, x5, x6, x7, x8] (Line 109452)\n",
    "\n",
    "\n",
    "\n",
    "(...)\n",
    "\n",
    "\n",
    "11)\n",
    "[x21, x22, x23, x24, x25, x26] (Line 1)\n",
    "[x21, x22, x23, x24, x25, x26] (Line 2)\n",
    "...\n",
    "[x21, x22, x23, x24, x25, x26] (Line 109452)\n",
    "\n",
    "------------------------------------\n",
    "[2n+1의 경우(25)]\n",
    "\n",
    "1)\n",
    "[x1, x2, x3, x4, x5, x6] (Line 1)\n",
    "[x1, x2, x3, x4, x5, x6] (Line 2)\n",
    "...\n",
    "[x1, x2, x3, x4, x5, x6] (Line 109452)\n",
    "\n",
    "2)\n",
    "[x3, x4, x5, x6, x7, x8] (Line 1)\n",
    "[x3, x4, x5, x6, x7, x8] (Line 2)\n",
    "...\n",
    "[x3, x4, x5, x6, x7, x8] (Line 109452)\n",
    "\n",
    "\n",
    "\n",
    "(...)\n",
    "\n",
    "\n",
    "11)\n",
    "[x21, x22, x23, x24, x25, pad] (Line 1)\n",
    "[x21, x22, x23, x24, x25, pad] (Line 2)\n",
    "...\n",
    "[x21, x22, x23, x24, x25, pad] (Line 109452)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample LSTM Tensorflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "biases = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "\n",
    "x = tf.transpose(x, [1, 0, 2])\n",
    "x = tf.reshpae(x, [-1, n_input])\n",
    "x = tf.split(0, n_steps, x )\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell( n_hidden, forget_bias=1.0)\n",
    "outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "pred = tf.matmul(outputs[-1], weights ) + biases\n",
    "\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "\n",
    "        sess.run(train, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"step : %d, acc: %f\" % ( step, acc )\n",
    "        step += 1\n",
    "    print \"train complete!\"\n",
    "\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"test accuracy: \", sess.run( accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
