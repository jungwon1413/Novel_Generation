{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generator using RNN\n",
    "Example script to generate text from Nietzsche's writings. At least 20 epochs are required before the generated text starts sounding coherent.<br>\n",
    "It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive. If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jungw\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import random    << use np.random instead.\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import chardet\n",
    "\n",
    "from collections import Counter\n",
    "from konlpy.tag import Twitter\n",
    "from scipy.sparse import *\n",
    "from tensorflow.python.keras.callbacks import LambdaCallback    # What is Lambda Callback??\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Activation\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.utils import get_file    # Do I have to use get_file instead of direct load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path, filename):\n",
    "    # path = get_file('path/to/text/file.txt', origin=None)\n",
    "    # filename = 'D:/Seed_Downloads/Novel Dataset/Asian fantasy/NOVEL_01001.txt'\n",
    "    if path[-1] is '/':\n",
    "        file = path + filename\n",
    "    else:\n",
    "        file = path + '/' + filename\n",
    "\n",
    "    read_file = open(file, 'rb').read()\n",
    "    encode_type = chardet.detect(read_file)['encoding']\n",
    "    text = open(file, encoding=encode_type).read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2928103\n",
      "First 100 characters: \n",
      "제 목:[검마전/ sword & magic story]-- 001.\n",
      "\n",
      "< 검 마 전 : sword & magic story >\n",
      "\n",
      "눈꺼풀이 무겁다. 머리도 띵하고. 눈을 떠야하는데.\n"
     ]
    }
   ],
   "source": [
    "text = load_file('D:/Seed_Downloads/Novel Dataset/Asian fantasy/', 'NOVEL_01001.txt')\n",
    "\n",
    "print('First 100 characters: {}'.format('\\n' + text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indicers(text):\n",
    "    temp_text = text\n",
    "    \n",
    "    temp_text = temp_text.replace(\" \", \" SPACE \")\n",
    "    temp_text = temp_text.replace(\"\\n\", \" ENTER \")\n",
    "    \n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed first 100 characters: \n",
      "제 SPACE 목:[검마전/ SPACE sword SPACE & SPACE magic SPACE story]-- SPACE 001. ENTER  ENTER < SPACE 검 SPA\n"
     ]
    }
   ],
   "source": [
    "text = remove_indicers(text)\n",
    "\n",
    "print('Fixed first 100 characters: {}'.format('\\n' + text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_word(text_replace):\n",
    "    twitter = Twitter()\n",
    "    text_split = twitter.morphs(text_replace)\n",
    "    words = sorted(list(set(text_split)))\n",
    "    min_word = min(words, key=len)\n",
    "    max_word = max(words, key=len)\n",
    "\n",
    "    print('Total words:', len(words))\n",
    "    print('min word is: {} with length of {}'.format(min_word, len(min_word)))\n",
    "    print('max word is: {} with length of {}'.format(max_word, len(max_word)))\n",
    "\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "    \n",
    "    return text_split, words, word_indices, indices_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 30211\n",
      "min word is: ! with length of 1\n",
      "max word is: \"@#%*(*^$#&*#$\" with length of 15\n",
      "Split check: 느낌\n",
      "Word check: ..]\n"
     ]
    }
   ],
   "source": [
    "text_split, words, word_indices, indices_word = split_to_word(text)\n",
    "\n",
    "print('Split check: {}'.format(text_split[100]))\n",
    "print('Word check: {}'.format(words[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_create(text_split):\n",
    "    sentences = []\n",
    "    temp = \"\"\n",
    "\n",
    "    twitter = Twitter()\n",
    "    \n",
    "    for i, word in enumerate(text_split):\n",
    "        temp = temp + word + ' '\n",
    "        if word == 'ENTER':\n",
    "            temp = twitter.morphs(temp)\n",
    "            sentences.append(temp)\n",
    "            temp = \"\"\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print('Total number of Sentences: {}'.format(len(sentences)))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Sentences: 109452\n",
      "Sentence check: ['제', 'SPACE', '목', ':[', '검', '마전', '/', 'SPACE', 'sword', 'SPACE', '&', 'SPACE', 'magic', 'SPACE', 'story', ']--', 'SPACE', '001', '.', 'ENTER']\n"
     ]
    }
   ],
   "source": [
    "sentences = sentence_create(text_split)\n",
    "\n",
    "print('Sentence check: {}'.format(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_sequence(sentences):\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "    longest_sentence = max(sentences)\n",
    "    maxlen = len(longest_sentence)\n",
    "    step = 3\n",
    "    next_words = []\n",
    "    sentence_data = []\n",
    "    seq_count_ = (len(text_split) - maxlen) // step\n",
    "\n",
    "    for i in range(0, len(text_split) - maxlen, step):\n",
    "        sentence_data.append(text_split[i: i + maxlen])    # nth char ~ n+maxlen char = Sentence\n",
    "        next_words.append(text_split[i + maxlen])    # next_chars = from ith char ~ end\n",
    "    \n",
    "    return sentence_data, next_words, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 682030\n",
      "101th sentence length: 26\n",
      "max length: 26\n"
     ]
    }
   ],
   "source": [
    "sentence_data, next_words, maxlen = generate_input_sequence(sentences)\n",
    "\n",
    "print('nb sequences:', len(sentence_data))\n",
    "print('101th sentence length: {}'.format(len(sentence_data[100])))\n",
    "print('max length: {}'.format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected memory to prepare: 19133036306.42857GB\n"
     ]
    }
   ],
   "source": [
    "Memory_to_use = len(sentence_data) * maxlen * len(words) / (2^30)\n",
    "\n",
    "print('Expected memory to prepare: {}GB'.format(Memory_to_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence check...\n",
      "Sentence 0:\n",
      "['귀', '를', 'SPACE', '기울여', '도', 'SPACE', '바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은']\n",
      "Sentence 1:\n",
      "['기울여', '도', 'SPACE', '바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은', '거', '?', 'SPACE']\n",
      "Sentence 2:\n",
      "['바람소리', '조차', 'SPACE', '들리', '지', 'SPACE', '않는', '다', '.', 'SPACE', '그럼', '..', 'SPACE', '설마', 'SPACE', '난', 'ENTER', '정말로', 'SPACE', '죽은', '거', '?', 'SPACE', '으아', '..!', 'SPACE']\n"
     ]
    }
   ],
   "source": [
    "print('Sequence check...')\n",
    "for i in range(3):\n",
    "    print('Sentence {}:\\n{}'.format(i, sentence_data[100+i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_indices(text_split):\n",
    "    indice_list = []\n",
    "    for i, word in enumerate(text_split):\n",
    "        add_indice = word_indices[word]\n",
    "        indice_list.append(add_indice)\n",
    "        \n",
    "    return indice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [23380, 1008, 11442, 987, 2873, 10386, 110, 1008, 1308, 1008, 25, 1008, 1199, 1008, 1303, 1012, 1008, 114, 66, 1007]\n",
      "length of encoded list: 2046114\n",
      "Encoded trigger word sample: [1008, 985, 1008, 1199, 1008, 1007, 1008, 66, 7990, 28452, 6645, 9258, 6794, 23511, 5959, 24023, 1008, 1008, 998, 21317]\n",
      "length of encoded list: 682030\n"
     ]
    }
   ],
   "source": [
    "encoded_text = word_to_indices(text_split)\n",
    "encoded_next_word = word_to_indices(next_words)\n",
    "\n",
    "print('Encoded text sample: {}'.format(str(encoded_text[:20])))\n",
    "print('length of encoded list: {}'.format(len(encoded_text)))\n",
    "print('Encoded trigger word sample: {}'.format(str(encoded_next_word[:20])))\n",
    "print('length of encoded list: {}'.format(len(encoded_next_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_zeros(sentence_data, encoded_next_word, maxlen, wordlen):\n",
    "    empty_input = np.zeros((10000, maxlen, wordlen), dtype=np.bool)\n",
    "    empty_output = np.zeros((10000, wordlen), dtype=np.bool)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    iter_count = 0\n",
    "        \n",
    "    for row, sentence in enumerate(sentence_data):\n",
    "        \n",
    "        if (row % 10000) is 0:\n",
    "            if row is not 0:\n",
    "                iter_count += 1\n",
    "                inputfile = 'B{:03d}X'.format(iter_count)\n",
    "                outputfile = 'B{:03d}Y'.format(iter_count)\n",
    "                \n",
    "                np.save(inputfile, empty_input)\n",
    "                np.save(outputfile, empty_output)\n",
    "                x_list.append(inputfile)\n",
    "                y_list.append(outputfile)\n",
    "                \n",
    "                print('Iteration #{} processed...'.format(iter_count))\n",
    "                \n",
    "                empty_input = np.zeros((10000, maxlen, wordlen), dtype=np.bool)\n",
    "                empty_output = np.zeros((10000, wordlen), dtype=np.bool)\n",
    "                \n",
    "        else:\n",
    "            row_count = row - iter_count * 10000\n",
    "            output_index = encoded_next_word[row]\n",
    "            empty_output[row_count, output_index] = 1\n",
    "\n",
    "            for col, word in enumerate(sentence):\n",
    "                input_index = word_indices[word]\n",
    "                empty_input[row_count, col, input_index] = 1\n",
    "                \n",
    "                \n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 processed...\n",
      "Iteration #2 processed...\n",
      "Iteration #3 processed...\n",
      "Iteration #4 processed...\n",
      "Iteration #5 processed...\n",
      "Iteration #6 processed...\n",
      "Iteration #7 processed...\n",
      "Iteration #8 processed...\n",
      "Iteration #9 processed...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "-735074592 requested and 0 written",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1fa35dc709a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_next_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-e59c520c5067>\u001b[0m in \u001b[0;36mnumpy_zeros\u001b[1;34m(sentence_data, encoded_next_word, maxlen, wordlen)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0moutputfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'B{:03d}Y'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mempty_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mempty_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mx_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 511\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    512\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m             \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[1;31mOSError\u001b[0m: -735074592 requested and 0 written"
     ]
    }
   ],
   "source": [
    "x_list, y_list = numpy_zeros(sentence_data, encoded_next_word, maxlen, len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 문제\n",
    "- 이슈: 너무 크다 (파일사이즈)\n",
    "    - 모든 방법을 이용해봤지만 여전히 너무 크다\n",
    "- 결론: Iteration을 통해 Feeding을 실시간으로 하는 방법을 사용한다\n",
    "\n",
    "<br>1. 원하는 Size의 Input/Output배열을 만든다\n",
    "<br>2. 실시간 생성을 통한 feeding 진행\n",
    "- 0~9999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- 10000 ~ 19999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- 20000 ~ 29999 생성\n",
    "<br> Train (0 Epoch)\n",
    "- ... (이하 생략)\n",
    "- 680000 ~ 682030 생성\n",
    "<br> Train (0 Epoch)\n",
    "<Br>3. Predict\n",
    "<br>4. Loss 산출\n",
    "<br>5. 2번의 과정을 다시 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(words))))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=25,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
